# -*- coding: utf-8 -*-
"""Final Deep Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZ81zSHzDOKQeFkVlyYY8WH6acTR4FQk

# Statistics:
1. Total number of images: 2,496,738
2. Number of real images: 964,989
3. Number of fake images: 1,531,749
4. Number of generators used for fake images: 25 (including 13 GANs, 7 Diffusion, and 5 miscellaneous generators)
5. Number of sources used for real images: 8
6. Categories included in the dataset: Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and other real-life objects
7. Image Resolution: 200 x 200

# Load Data
"""

# Download kaggle.json file
from google.colab import files
files.upload()

# Move the kaggle.json file to the correct folder
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d birdy654/cifake-real-and-ai-generated-synthetic-images

!unzip cifake-real-and-ai-generated-synthetic-images.zip

"""# Just to learn about the data set"""

import os
import shutil
import re
from tqdm import tqdm
from collections import defaultdict
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image

# Define your source and destination folders
source_dirs = ['train/FAKE', 'train/REAL', 'test/FAKE', 'test/REAL']
dest_root = 'organized_dataset_by_class'

# Regex to extract class number
pattern = re.compile(r'\((\d+)\)')

# Helper function to extract class from filename
def get_class_number(filename):
    match = pattern.search(filename)
    if match:
        return int(match.group(1))
    else:
        return 1

# Dictionary to count images per class
counter = defaultdict(int)

# Start organizing
for src in tqdm(source_dirs, desc="Organizing dataset"):
    # Determine if FAKE or REAL
    label = 'FAKE' if 'FAKE' in src else 'REAL'

    # Go through all files
    for filename in os.listdir(src):
        if filename.endswith('.jpg') or filename.endswith('.png'):
            class_num = get_class_number(filename)
            dest_folder = os.path.join(dest_root, label, f'class_{class_num}')
            os.makedirs(dest_folder, exist_ok=True)
            src_path = os.path.join(src, filename)
            dest_path = os.path.join(dest_folder, filename)
            shutil.copy(src_path, dest_path)

            # Update counter
            counter[(label, class_num)] += 1

# Print summary
print("\nSummary Report:")
for (label, class_num), count in sorted(counter.items()):
    print(f"{label} - Class {class_num}: {count} images")

# Show one random image from each class
print("\nShowing one image from each class:")

for (label, class_num) in sorted(set(counter.keys())):
    folder_path = os.path.join(dest_root, label, f'class_{class_num}')
    images = os.listdir(folder_path)
    if images:
        random_image = random.choice(images)
        img_path = os.path.join(folder_path, random_image)
        img = mpimg.imread(img_path)
        plt.figure(figsize=(3,3))
        plt.imshow(img)
        plt.title(f'{label} - Class {class_num}')
        plt.axis('off')
        plt.show()

import os

train_dir = '/content/train'
test_dir = '/content/test'

#  لحساب عدد الصور في المجلدات
def count_images_in_dir(directory):
    image_count = 0
    for subdir, _, files in os.walk(directory):
        for file in files:
            if file.endswith(('.jpg', '.png', '.jpeg')):
                image_count += 1
    return image_count

# حساب الصور في train و test
train_images = count_images_in_dir(train_dir)
test_images = count_images_in_dir(test_dir)

print(f"Number of images in train: {train_images}")
print(f"Number of images in test: {test_images}")

"""# Some Functions"""

import plotly.graph_objects as go

def plot_training_history_with_plotly(history):
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=list(range(1, len(history.history['accuracy']) + 1)),
        y=history.history['accuracy'],
        mode='lines',
        name='Train Accuracy'
    ))

    fig.add_trace(go.Scatter(
        x=list(range(1, len(history.history['val_accuracy']) + 1)),
        y=history.history['val_accuracy'],
        mode='lines',
        name='Validation Accuracy'
    ))

    fig.update_layout(
        title='Model Accuracy',
        xaxis_title='Epoch',
        yaxis_title='Accuracy',
        legend=dict(x=0, y=1),
    )

    fig.show()

    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=list(range(1, len(history.history['loss']) + 1)),
        y=history.history['loss'],
        mode='lines',
        name='Train Loss'
    ))

    fig.add_trace(go.Scatter(
        x=list(range(1, len(history.history['val_loss']) + 1)),
        y=history.history['val_loss'],
        mode='lines',
        name='Validation Loss'
    ))

    fig.update_layout(
        title='Model Loss',
        xaxis_title='Epoch',
        yaxis_title='Loss',
        legend=dict(x=0, y=1),
    )

    fig.show()

"""# Augmentation

"""

from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import os
import matplotlib.pyplot as plt
from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import ModelCheckpoint

img_path = '/content/train/REAL/0000 (3).jpg'
img = Image.open(img_path)

img_array = np.array(img)
print(img_array.shape)

import os
import random
import shutil

train_dir = '/content/train'         # مجلد train فيه Real و Fake
val_dir = '/content/validation'      # مجلد جديد حننشئه لو مش موجود

# التأكد من إنشاء مجلد validation
os.makedirs(val_dir, exist_ok=True)

classes = ['REAL', 'FAKE']
val_ratio = 0.2  # نسبة 20% للـ validation

for cls in classes:
    cls_train_path = os.path.join(train_dir, cls)
    cls_val_path = os.path.join(val_dir, cls)
    os.makedirs(cls_val_path, exist_ok=True)

    all_images = [
        fname for fname in os.listdir(cls_train_path)
        if os.path.isfile(os.path.join(cls_train_path, fname))
    ]

    # Random selection of 10% of images
    val_count = int(len(all_images) * val_ratio)
    selected_for_val = random.sample(all_images, val_count)

    print(f'📦 {cls}: نقل {val_count} صورة من أصل {len(all_images)} إلى validation')

    # Transfer images
    for fname in selected_for_val:
        src = os.path.join(cls_train_path, fname)
        dst = os.path.join(cls_val_path, fname)
        shutil.move(src, dst)

# حساب عدد الfake وال real في ال train
import os


class_counts = {}

for class_name in os.listdir(train_dir):
    class_path = os.path.join(train_dir, class_name)
    if os.path.isdir(class_path):
        count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])
        class_counts[class_name] = count

# Print the results
for class_name, count in class_counts.items():
    print(f"{class_name}: {count} صورة")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 1.ImageDataGenerators

# Augmentation + Normalization للـ train
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# فقط Normalize للـ val و test
val_test_datagen = ImageDataGenerator(rescale=1./255)

# 2.Load data from folders

train_path = '/content/train'
val_path = '/content/validation'
test_path = '/content/test'

# train data
train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(32, 32),
    batch_size=32,
    class_mode='binary',
    shuffle=True
)

# validation data
val_generator = val_test_datagen.flow_from_directory(
    val_path,
    target_size=(32, 32),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

# test data
test_generator = val_test_datagen.flow_from_directory(
    test_path,
    target_size=(32, 32),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

"""# Modeling

"""

!pip install visualkeras

from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import os
import matplotlib.pyplot as plt
from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D,Flatten,MaxPooling2D,BatchNormalization, Dropout
import visualkeras

# تحدد وين بدك تحفظ الموديل وشو تراقب
checkpoint = ModelCheckpoint(
    filepath='best_model.keras',    # اسم الملف اللي بدك تحفظ فيه
    monitor='val_accuracy',          # الشيء اللي بدك تراقبه (مثلا أقل قيمة لخسارة الفاليديشن)
    mode='max',                  # لما تكون بدك أقل خسارة (min) أو أعلى دقة (max)
    save_best_only=True,         # يحفظ بس لما يتحسن
    verbose=1                    # يظهرلك كل مرة يحفظ فيها
)

"""## CIFAKE Model"""

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))
model.add(MaxPooling2D())
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D())
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early = callbacks.EarlyStopping(monitor='val_accuracy', verbose=1, mode='auto', restore_best_weights=True, patience=5)

# Display model summary
model.summary()

model.build()
visualkeras.layered_view(model, scale_xy=5 ,legend=True)

# fitting the model
history = model.fit(
    train_generator,               # Training data with Augmentation
    epochs=100,                     # number of Epochs
    validation_data=val_generator,  # validation data without Augmentation
    verbose=1 ,
    callbacks=[early,checkpoint]
)

model.save('CIFAKE.keras')

"""### Evaluation of Model1 "CIFAKE"
"""

from tensorflow.keras.models import load_model
model = load_model('/content/CIFAKE.keras')

val_loss, val_accuracy = model.evaluate(test_generator)
print(f'📊 Test Accuracy: {val_accuracy*100:.2f}%')
print(f'💥 Test Loss: {val_loss:.4f}')
# accuracy: 0.9599 - loss: 0.1058
#📊 Test Accuracy: 89.89%
#💥 Test Loss: 0.2576

plot_training_history_with_plotly(history)

"""## CIFAKE2"""

model2 = Sequential()
model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))
model2.add(MaxPooling2D(padding='same'))
model2.add(BatchNormalization())
model2.add(Conv2D(32, (3, 3), activation='relu'))
model2.add(MaxPooling2D(padding='same'))
model2.add(Dropout(0.3))
model2.add(Flatten())
model2.add(Dense(64, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))  # لو كان عندك تصنيف ثنائي

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early2 = callbacks.EarlyStopping(monitor='val_accuracy', verbose=1, mode='auto', restore_best_weights=True, patience=7)

# Display model summary
model2.summary()

model2.build()
visualkeras.layered_view(model2, scale_xy=5 ,legend=True)

model2_history = model2.fit(
    train_generator,
    epochs=100,
    batch_size= 32,
    validation_data=val_generator,
    verbose=1 ,
    callbacks=[early2,checkpoint]
)

model.save('CIFAKE2.keras')

"""### Evaluation of Model2 "CIFAKE2"
"""

model2 = load_model('/content/CIFAKE2.keras')

val_loss, val_accuracy = model2.evaluate(test_generator)
print(f'📊 Test Accuracy: {val_accuracy*100:.2f}%')
print(f'💥 Test Loss: {val_loss:.4f}')
# accuracy: 0.9528 - loss: 0.1224
#📊 Test Accuracy: 88.82%
#💥 Test Loss: 0.2832

plot_training_history_with_plotly(model2_history)

"""# Some Visualization"""

models = {
    'Model_1': model,
    'Model_2': model2
}

for name, model in models.items():
    print(f"Results for {name}:")

    y_pred_probs = model.predict(test_generator)
    y_pred = (y_pred_probs > 0.5).astype(int).flatten()
    y_true = test_generator.classes

    print(classification_report(y_true, y_pred, target_names=['FAKE', 'REAL']))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['FAKE', 'REAL'], yticklabels=['FAKE', 'REAL'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix for {name}')
    plt.show()

import os

models = {
    'Model_1': model,
    'Model_2': model2
}

y_true = test_generator.classes
class_labels = list(test_generator.class_indices.keys())
filenames = test_generator.filenames

for name, model in models.items():
    print(f"\nShowing wrong predictions for {name}:")

    y_pred_probs = model.predict(test_generator)
    y_pred = (y_pred_probs > 0.5).astype(int).flatten()

    wrong_preds = np.where(y_pred != y_true)[0]

    plt.figure(figsize=(15, 10))
    for i, idx in enumerate(wrong_preds[:6]):  # اعرض أول 6 أغلاط
        img_path = os.path.join(test_generator.directory, filenames[idx])
        img = plt.imread(img_path)

        true_label = class_labels[y_true[idx]]
        pred_label = class_labels[y_pred[idx]]

        plt.subplot(3, 3, i+1)
        plt.imshow(img)
        plt.axis('off')
        plt.title(f'🚫 True: {true_label} | Pred: {pred_label}', color='black')

    plt.suptitle(f'Examples of mistakes - {name}', fontsize=16)
    plt.tight_layout()
    plt.show()